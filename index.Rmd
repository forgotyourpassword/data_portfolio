---
title: "Data Analytics Portfolio"
author: "Michael Molloy"
date: "`r Sys.Date()`"
output:
  html_document:
    code_folding: show
    highlight: tango
    number_sections: yes
    theme: cerulean
    toc: yes
    toc_depth: 3
    toc_float: yes
    includes:
          after_body: "./assets/html/footer.html"
    
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
# Home

# Data Analysis
```{r include = FALSE}
library(knitr)
library(rmdformats)
library(dplyr)
library(ggplot2)
library(kableExtra)
library(stringdist)
library(reshape2)
library(stringr)
library(wordcloud2)
library(tm)
library(usmap)
library(lubridate)
library(tidyverse)


# Define multmerge function - merges multiple csv files together
multmerge = function(mypath) {
  filenames=list.files(path=mypath, full.names = TRUE)
  datalist = lapply(filenames, function(x){
    read_csv(file=x)
  })
  Reduce(function(x,y){
    merge(x,y,all = TRUE)
  }, datalist)
}
# Load files
breaches = multmerge("./input/source/hipaa_breaches/source/")
entity_name_map = read.csv("./input/source/hipaa_breaches/project/entity_name_map.csv",stringsAsFactors = F)

# clean up the dates
breaches <- breaches %>% 
 mutate(date = mdy(breaches$`Breach Submission Date`),
         year = year(date),
         month = floor_date(date, unit = "month"))
```

## HIPAA Breaches
In just the first four months of 2019, `r NROW(breaches[str_detect(breaches$Breach.Submission.Date,'2019'),"Breach.Submission.Date"])` breaches were reported to the Office for Civil Rights. In total these breaches affected `r format(sum(breaches[str_detect(breaches$Breach.Submission.Date,'2019'),"Individuals.Affected"]), big.mark = ",")` individuals.  Even with patient privacy being a concern for many health providers the industry is falling short in protecting sensitive data. Across the US, companies are reporting incidents of theft, unauthorized access, and hacking. 

### Breaches over time
The Office for Civil Rights publishes a dataset of reported breaches going back as far as 2009.  If we graph the number of breaches reported each month over the past 9 years the trend is clear.  We can also see a few months with a high number of "Individuals Affected" around 2015 (represented by the size of the dot). The most notable being the Anthem breach reported in February 2015 which affected 78,800,000 people.
```{r echo=FALSE, message=FALSE, warning=FALSE}
individuals_by_month <- breaches %>% 
   group_by(year,month) %>% 
  summarize(total_records = sum(na.omit(`Individuals Affected`)))

count_by_month <- breaches %>% 
    group_by(year,month) %>% 
  summarize(count = n())

breaches_by_month <- merge (individuals_by_month, count_by_month)

breaches_by_month %>% 
  filter(year != 2019) %>% 
  ggplot(aes(month, count, size = total_records)) +
    geom_point(color = "purple") +
   scale_x_date(NULL, date_labels = "%Y", breaks = "year") +
    geom_smooth(method = "lm", show.legend= FALSE) +
  scale_size_continuous(breaks = c(20000000,40000000,60000000),labels = c("20M","40M","60M")) +
  labs(title = "Number of Breaches by Month", y = "# of breaches", x = "Year", size = "# of records") 
```

### Types of Breaches
The most common types of breaches are hacking, theft, and unauthorized access. Breach repors can contain multiple breach types, so some parsing had to be done in order to create a meaningful chart on this field.  The graph below shows the number of reported breaches per month distinguished by their breach type using color.  The size of the dot, similar to our previous chart, shows the relative sum of the records (i.e. individuals affected) for the breaches in the month.  

A few things stand out in this chart. Most notably that the reporting of theft has been declining sharply and the reports of unauthorized access and hacking have been increasing. My hypothesis on why the reports of theft have dropped is that the prevalence of encrypted storage has increased.  This is a purely anecdotal observation from my experience in IT Risk over the past 10 years, but it makes sense. If a laptop is stolen but its hard drive is encrypted, OCR does not consider the data on that hard drive accessible to an unauthorized user and therefore would not constitute a reportable breach. 

My other observation is that the biggest breaches (i.e. ones that affect the most individuals) are related to hacking. When you combine that with the increase in hacking incidents it paints a dire picture for individual privacy. 

```{r echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE}
hack_by_month <- breaches %>% 
  filter(str_detect(`Type of Breach`, "Hack")) %>% 
  group_by(year,month) %>% 
  summarize(count = n(), records = sum(`Individuals Affected`)) %>% 
  mutate(type = "Hack")

theft_by_month <- breaches %>% 
  filter(str_detect(`Type of Breach`, "Theft")) %>% 
  group_by(year,month) %>% 
  summarize(count = n(), records = sum(`Individuals Affected`)) %>% 
  mutate(type = "Theft")

access_by_month <- breaches %>% 
  filter(str_detect(`Type of Breach`, "Access")) %>% 
  group_by(year,month) %>% 
  summarize(count = n(), records = sum(`Individuals Affected`)) %>% 
  mutate(type = "Unauthorized Access")
# 
# loss_by_month <- breaches %>% 
#   filter(str_detect(Type.of.Breach, "Loss")) %>% 
#   group_by(year,month) %>% 
#   summarize(count = n())%>% 
#   mutate(type = "Loss")
# 
# disposal_by_month <- breaches %>% 
#   filter(str_detect(Type.of.Breach, "Improper")) %>% 
#   group_by(year,month) %>% 
#   summarize(count = n()) %>% 
#   mutate(type = "Improper Disposal")
# 
# other_by_month <- breaches %>% 
#   filter(str_detect(Type.of.Breach, "Other")) %>% 
#   group_by(year,month) %>% 
#   summarize(count = n()) %>% 
#   mutate(type = "Other")

type_by_month <- rbind(hack_by_month,theft_by_month,access_by_month)

type_by_month %>% 
  ggplot(aes(month, count, color = type, size = records)) +
  geom_point(show.legend = TRUE) +
  scale_x_date(breaks = "year", date_labels = "%Y") +
  ylim(0,25) +
  geom_smooth(method="lm", show.legend = FALSE) +
  scale_size_continuous(breaks = c(20000000,40000000,60000000),labels = c("20M","40M","60M")) +
  labs(title = "Types of Data Breaches", color = "Breach Type", x = "Year", y = "# of breaches per month", size = "# of records")
```


### Breach Map {.tabset}
#### Total Breaches by State
The map below shows the total number of breaches by state. At first glance, it appears as though covered entities in California, Texas, and Florida have much higher occurrences of data breaches than covered entities in other states.  However, this map looks a lot like a map of state populations from the 2015 census (see the next tab).
```{r echo=FALSE}
# count numer of breaches by state
dfState <- breaches %>%
  group_by("state" = State) %>%
  summarise("Total" = length(State)) %>%
  arrange(desc(Total))

# plot total breaches on map
plot_usmap(data = dfState, values = "Total",regions = "states", lines = "white") + 
  scale_fill_continuous(name = "Breaches", label = scales::comma, high = "red", low = "white") + 
  theme(legend.position = "right") +
  labs(title = "Breaches by State", subtitle = "From Oct 2009- Apr 2019", caption = "Office of Civil Rights, https://ocrportal.hhs.gov/ocr/breach/breach_report.jsf")
```

#### Population Map
There appears to be a correlation between the number of breaches in each state and the population.  It might be worth exploring this further. I'd want to test the hypothesis that a higher population means more covered entities (i.e. hospitals) which are available to be breached in a given state. The next tab shows a more normal distribution of breaches across the US.
```{r echo=FALSE}
plot_usmap(data = statepop, values = "pop_2015", lines = "white") + 
  scale_fill_continuous(name = "Population", label = scales::comma, high = "red", low = "white") + 
  theme(legend.position = "right") +
  labs(title = "Population by State", subtitle = "2015",caption = "United States Census")
```

#### Breaches by Population
I normalized the breaches by dividing the number of breaches by the total state population. I expect this represents a more accurate picture, but statistical analysis would need to be done to determine whether a particular state is more likely to have a breach than others.
```{r echo=FALSE}
dfState <- breaches %>%
  group_by("abbr" = State) %>%
  summarise("Breaches" = length(State)) %>%
  arrange(desc(Breaches))

breach_pop <- merge(dfState, statepop, by = "abbr")
breach_pop$Breach.Percent <- as.numeric(breach_pop$Breaches/breach_pop$pop_2015, digits = 2, format = "f")*1000000

plot_usmap(data = breach_pop, values = "Breach.Percent",regions = "states", lines = "white") + 
  scale_fill_continuous(name = "Ratio", label = scales::comma, high = "red", low = "white") + 
  theme(legend.position = "right") +
  labs(title = "Breaches as a Ratio of Population", subtitle = "Breaches from Oct 2009- Apr 2019/ State Populations from 2015 US Census")
```


### Covered Entities {.tabset}
#### Most Breached Entities
I also wanted to see which covered entities incurred the most breaches. I found quality issues with the covered entity name column.  I used a clustering algorithm to automatically group like names, then created a "mapping table" (see other tab) of rules based on those clusters. After running the mapping table logic, I was able to count all the breaches by entity and get a more accurate picture of the covered entities with the most reported breaches.

```{r echo=FALSE}
# change covered entity names to a common name
for (row in 1:NROW(entity_name_map)) {
  from <- entity_name_map[row,1]
  to <- entity_name_map[row,2]
  breaches$`Name of Covered Entity` <- str_replace_all(breaches$`Name of Covered Entity`,from,to)
}

# Count the number of occurences for each Entity
dfName <- breaches %>%
  group_by("Entity Name" = `Name of Covered Entity`) %>%
  summarise("Total" = length(`Name of Covered Entity`)) %>%
  arrange(desc(Total))

head(dfName,20) %>% 
ggplot(aes(reorder(`Entity Name`,Total),Total, fill = as.factor(Total))) +
  geom_col() +
  guides(fill=FALSE)+
  coord_flip() +
  ylim(0,25) +
  labs(title = "Entities with the most breaches", x = "", y = "# of breaches")
```

#### Mapping Table

```{r echo=FALSE}
# print table
kable(entity_name_map) %>%
  kable_styling(bootstrap_options = c("striped","hover", "condensed","responsive"),fixed_thead = T) %>%
  column_spec(1, bold = T, border_right = T) %>%
  column_spec(2)
```

### Conclusion
Cyber criminals are attacking organizations of all sizes and industry verticals as IT and Security departments struggle to keep up with the latest threats.  The Healthcare industry has seen an increase in the number of breaches over the past few years, especially those related to hacking.  Understanding the threat landscape is an important part of increasing the privacy protections over patient data and identifying solutions that reduce breaches.  

# Code Examples

## Clustering
Clustering allows for more automated pattern recognition within datasets. I found it helpful in identifying instances where company names were entered with slight variations (i.e. "Clearview", "ClearView Inc.", "Clearview Group"), making grouping and counting impossible. We can use clustering to identify the similarities and write data wrangling scripts to tidy up the data for further analysis.

### Illustrative Example
The example below uses board games ratings data.  Clustering can be used to identify games with similar names.

```{r echo=TRUE, message=FALSE, warning=FALSE}
# Load packages
library(tidyverse)
library(dplyr)
library(ggplot2)
library(kableExtra)
library(stringdist)
library(reshape2)
library(stringr)
library(tm)
library(wordcloud2)

# read data
data <- readr::read_csv("https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-03-12/board_games.csv")

# select the field in the dataset with which you want to perform clustering
Field <- head(data$name, 1000)

# create the stringdistmatrix using the Jaro–Winkler distance algorithm 
UniqueField <- unique(as.character(Field))

DistanceField <- stringdistmatrix(UniqueField,UniqueField,method = "jw") # you can experiment with changing the method to get better results

rownames(DistanceField) <- UniqueField

# loop the clustering algorithm until it reaches the desired average number of records. the thought being that if we have a dataset with records we expect to be mostly unique than the average records in a cluster will be low.  This loop will determine the right number of clusters for your dataset.

  # set starting number of clusters
  i <- 1
  
  # set a starting avg_cluster (should be greater than x)
  avg_cluster <- 10
  
  # this one is important. how many records do you expect to be similar?
  x <- 2
  
  # define variable for use in clustering algorithm
  hc <- hclust(as.dist(DistanceField))

  # begin the loop
  while (avg_cluster > x) {
     dfClust <- data.frame(UniqueField, cutree(hc, k=i))
     names(dfClust) <- c('UniqueField','cluster')
     avg_cluster <- mean(table(dfClust$cluster))
     i = i+1
     }

# compile the cluster data into a data.frame
t <- table(dfClust$cluster)
t <- cbind(t,t / length(dfClust$cluster))
t <- t[order(t[,2], decreasing=TRUE),]
p <- data.frame(factorName=rownames(t), binCount=t[,1], percentFound=t[,2])


dfClust <- merge(x=dfClust, y=p, by.x = 'cluster', by.y='factorName', all.x=T)

dfClust <- dfClust[rev(order(dfClust$binCount,dfClust$cluster)),] # sort by the size of the cluster bin then the cluster ID

names(dfClust) <-  c('cluster','UniqueField','binCount')
```


The relationship between the number of clusters and the total number of records is interesting. In theory, if a dataset has no discernable patterns with which to cluster on, then the number of clusters would be equal to the number of records. In other words, each record would be its own "cluster".
```{r echo=TRUE, message=FALSE, warning=FALSE}
# How many clusters did we end up with?
print(paste('Number of clusters:', i))

# How many records in the dataset?
print(paste('Number of records:',NROW(data)))
```

Now we can take a look at the top results to start identifying the "real" patterns and further refining our data.
```{r echo=TRUE, message=FALSE, warning=FALSE}
# print cluster results
kable(dfClust[order(dfClust[1:200,3], dfClust[1:200,2], decreasing = T),1:2]) %>%
  kable_styling(bootstrap_options = c("striped","hover", "condensed","responsive"),fixed_thead = T) %>%
  column_spec(1, bold = T, border_right = T) %>%
  column_spec(2, width = "5m")
```

## Word Clouds
Word clouds are an interesting way to visualize the frequency of words in a dataset. 

### Packages
The "tm" package helps to clean up the data by removing special characters, punctuation, etc. The "wordcloud2" package is used to display the most common words visually in the form of a word cloud.

### Illustrative Example
The example below contains a word cloud for the description of UFO sightings collected by THE NATIONAL UFO REPORTING CENTER. 

```{r echo=TRUE, message=FALSE, warning=FALSE}

# Load packages
library(tidyverse)
library(dplyr)
library(ggplot2)
library(kableExtra)
library(stringdist)
library(reshape2)
library(stringr)

# read data
data <- readr::read_csv("https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-06-25/ufo_sightings.csv")

# select the field in the dataset with which you want to create the word cloud.
Field <- head(data$description, 1000) # for illustrative purpose only the first 1000 rows are used.


# the "tm" package cleans up the data in the field be seperating all of the words and normalizing them
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
Field2 <- as.character(Field)
Field2 <- Corpus(VectorSource(Field2))
Field2 <- tm_map(Field2, toSpace, "/")
Field2 <- tm_map(Field2, toSpace, "@")
Field2 <- tm_map(Field2, toSpace, "\\|")
Field2 <- tm_map(Field2, content_transformer(tolower))
Field2 <- tm_map(Field2, removeNumbers)
Field2 <- tm_map(Field2, removePunctuation)
Field2 <- tm_map(Field2, stripWhitespace)
Field2 <- tm_map(Field2, removeWords, stopwords("english"))

# this section of code takes all of the individual words and determines their frequency
dtm <- TermDocumentMatrix(Field2)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)

# this creates the word cloud image
wordcloud2(d[1:50,1:2])
```

# R for Data Science
## Intro
To continue expanding my skills in R, I worked through the entirety of Garrett Grolemund and Hadley Wickham's book R for Datascience (https://r4ds.had.co.nz/index.html).  The section below contains all of the exercises and notes I took while going through the book.

## Data Visualization
### First Steps
1. Run ggplot(data = mpg). What do you see?
```{r}
ggplot(data = mpg)
```
Running this code does not produce a graph.

2. How many rows are in mpg? How many columns?
```{r}
#rows
NROW(mpg)

#columns
NCOL(mpg)
```

3. What does the drv variable describe? Read the help for ?mpg to find out.
```{r}
?mpg
```
The drv variable describes the drivetrain of the vehicle. f = front-wheel drive, r = rear wheel drive, 4 = 4wd

4. Make a scatterplot of hwy vs cyl.
```{r}
ggplot(data = mpg) + 
  geom_point(mapping = aes(x = hwy, y = cyl))
```

5. What happens if you make a scatterplot of class vs drv? Why is the plot not useful?
```{r}
ggplot(data = mpg) + 
  geom_point(mapping = aes(x = class, y = drv))
```
This plot is not useful because neither variable is numeric. There is no discernable pattern.

### Aesthetic Mappings
1. What’s gone wrong with this code? Why are the points not blue?
```{r}
ggplot(data = mpg) + 
  geom_point(mapping = aes(x = displ, y = hwy, color = "blue")) +
  labs(title="Wrong")

ggplot(data = mpg) + 
  geom_point(mapping = aes(x = displ, y = hwy), color = "blue") +
  labs(title="Correct")
```
The color aesthetic was in the aes() so it was treated as a variable.

2. Which variables in mpg are categorical? Which variables are continuous? (Hint: type ?mpg to read the documentation for the dataset). How can you see this information when you run mpg?
```{r}
?mpg
str(mpg)

```
Categorical | Continious
------------|-----------
manufacturer| cty
model       | hwy
trans       | year
drv         | cyl
fl          |
class       |

3. Map a continuous variable to color, size, and shape. How do these aesthetics behave differently for categorical vs. continuous variables?

```{r}
ggplot(data = mpg) +
  geom_point(aes(x = class, y = manufacturer,size = cyl,shape = drv, color = hwy))
```


4. What happens if you map the same variable to multiple aesthetics?
```{r}
ggplot(data = mpg) +
  geom_point(aes(x = class, y = manufacturer,size = drv,shape = drv, color = drv))
```

When mapping a single variable to multiple aesthetics, R merges them into one image and puts it in one legend.

5. What does the stroke aesthetic do? What shapes does it work with? (Hint: use ?geom_point)
```{r}
?geom_point

ggplot(data = mpg) +
  geom_point(aes(x = class, y = manufacturer,stroke = 5, color=hwy))
```


6. What happens if you map an aesthetic to something other than a variable name, like aes(colour = displ < 5)? Note, you’ll also need to specify x and y.

```{r}
ggplot(mpg) +
  geom_point(aes(x = class, y= hwy,colour = displ < 5),size=3)
```

When you use an operator in an aesthetic R will use that aesthetic to display the results of the operator. In the example above, the color is set to blue when the displacement is greater than 5 and red when it is not.

### Facets
1. What happens if you facet on a continuous variable?
```{r}
ggplot(data = mpg) + 
  geom_point(mapping = aes(x = displ, y = drv)) +
  facet_grid(~hwy)
```
Faceting on a continous variable results in too many facets to be practical and useful.

2. What do the empty cells in plot with facet_grid(drv ~ cyl) mean? How do they relate to this plot?

```{r}
ggplot(data = mpg) + 
  geom_point(mapping = aes(x = drv, y = cyl)) +
facet_grid(drv ~ cyl)
```

3. What plots does the following code make? What does . do?

```{r}
ggplot(data = mpg) + 
  geom_point(mapping = aes(x = displ, y = hwy)) +
  facet_grid(drv ~ .)

ggplot(data = mpg) + 
  geom_point(mapping = aes(x = displ, y = hwy)) +
  facet_grid(. ~ cyl)
```
The . facets on a single variable instead of in rows and columns.

4. Take the first faceted plot in this section:
```{r}
ggplot(data = mpg) + 
  geom_point(mapping = aes(x = displ, y = hwy)) + 
  facet_wrap(~ class, nrow = 2)
```

What are the advantages to using faceting instead of the colour aesthetic? What are the disadvantages? How might the balance change if you had a larger dataset?

The advantage of using faceting instead of the color aesthetic is you can distinguish patterns more clearly. The disadvantage is faceting takes up more space. If you have a larger dataset, you might use geom_smooth to show trends instead of individual data points.

5. Read ?facet_wrap. What does nrow do? What does ncol do? What other options control the layout of the individual panels? Why doesn’t facet_grid() have nrow and ncol arguments?

nrow and ncol are used to manually set the number of rows and columns the facets should fit into. Another useful option is scales, which can be set to "free" when faceting data that has drastically different ranges. Facet_grid doesn't allow for setting of nrow and ncol because the variables determine this.

6. When using facet_grid() you should usually put the variable with more unique levels in the columns. Why?

Most displays are wider than they are tall.

###





